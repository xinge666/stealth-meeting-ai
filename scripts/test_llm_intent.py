#!/usr/bin/env python3
"""
Precision Test for LLM-based IntentRouter.
Verifies both classification (is_question) and extraction (cleaned text).
"""

import asyncio
import sys
import os

# Add root to sys.path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.config import AppConfig
from src.event_bus import EventBus, EventType, speech_event
from src.intelligence.llm_client import LLMClient
from src.intelligence.intent_router import IntentRouter
from src.context import ContextManager

TEST_CASES = [
    {
        "input": "å‘ƒï¼Œé‚£ä¸ªï¼Œæˆ‘æƒ³è¯·é—®ä¸€ä¸‹ï¼Œå°±æ˜¯ Transformer é‡Œé¢é‚£ä¸ª Self-Attention ä¸ºä»€ä¹ˆè¦é™¤ä»¥æ ¹å· dk å‘€ï¼Ÿ",
        "expected_is_question": True,
        "desc": "Noisy interview question"
    },
    {
        "input": "å¥½çš„ï¼Œé‚£æ¥ä¸‹æ¥æˆ‘ä»¬èŠèŠé‚£ä¸ªï¼Œå‘ƒï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ RNN æ¥è¯´ï¼ŒTransformer åœ¨å¹¶è¡Œè®­ç»ƒä¸Šåˆ°åº•æœ‰ä»€ä¹ˆæ ¸å¿ƒä¼˜åŠ¿ï¼Ÿ",
        "expected_is_question": True,
        "desc": "Question mixed with filler"
    },
    {
        "input": "å—¯ï¼Œæˆ‘è§‰å¾—ä½ è¯´å¾—æŒºå¯¹çš„ï¼Œæ²¡å…³ç³»ï¼Œæˆ‘ä»¬ç»§ç»­å§ã€‚",
        "expected_is_question": False,
        "desc": "Conversational feedback"
    },
    {
        "input": "ä½ å¥½å¤§å®¶å¥½ï¼Œå¾ˆé«˜å…´æ¥åˆ°è¿™é‡Œé¢è¯•ï¼Œæˆ‘æ˜¯å¼ ä¸‰ã€‚",
        "expected_is_question": False,
        "desc": "Self-introduction noise"
    },
    {
        "input": "è¯·å¸®æˆ‘è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æ®‹å·®è¿æ¥ï¼Œä»¥åŠå®ƒåœ¨æ·±å±‚ç½‘ç»œä¸­è§£å†³äº†ä»€ä¹ˆé—®é¢˜ã€‚",
        "expected_is_question": True,
        "desc": "Clear technical question"
    }
]

async def run_precision_test():
    config = AppConfig.from_env()
    bus = EventBus()
    llm = LLMClient(config.llm, bus)
    await llm.initialize()

    flash_llm = LLMClient(config.flash_llm, bus)
    await flash_llm.initialize()
    
    context_mgr = ContextManager(bus)
    router = IntentRouter(bus, flash_llm, context_mgr)
    
    captured_results = []

    async def on_question(event):
        captured_results.append({
            "text": event.data.get("text"),
            "confidence": event.data.get("confidence")
        })

    bus.subscribe(EventType.INTENT_QUESTION, on_question)
    await bus.start()

    print("\n" + "="*70)
    print("ğŸ¯ LLM Intent Precision & Extraction Test")
    print("="*70 + "\n")

    passed = 0
    for case in TEST_CASES:
        text = case["input"]
        expected_q = case["expected_is_question"]
        
        captured_results.clear()
        
        print(f"ğŸ“¥ Input: {text}")
        await bus.publish(speech_event(text))
        
        # LLM call takes time
        await asyncio.sleep(2.5) 
        
        found_q = len(captured_results) > 0
        
        status = "âŒ FAIL"
        if found_q == expected_q:
            status = "âœ… PASS"
            passed += 1
            
        if found_q:
            extracted = captured_results[0]["text"]
            conf = captured_results[0]["confidence"]
            print(f"{status} | Found [Q] (conf={conf:.2f}): \"{extracted}\"")
        else:
            print(f"{status} | No question detected.")
        print("-" * 70)

    print(f"\nğŸš€ Final Result: {passed}/{len(TEST_CASES)} Passed")
    print("="*70 + "\n")

    await bus.stop()
    await llm.close()

if __name__ == "__main__":
    asyncio.run(run_precision_test())
